{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PytorchTutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPWiJ5FWiSF2/j9h1cMN8Oa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gokulanv/LeNet_Pytorch/blob/master/LeNetPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju9hoASHfQOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmTYbadS3f_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "edc6f674-9904-46c5-d2e1-b2c067afc094"
      },
      "source": [
        "from torch import FloatTensor\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "# Define the leaf nodes\n",
        "a = Variable(FloatTensor([4]))\n",
        "\n",
        "weights = [Variable(FloatTensor([i]), requires_grad=True) for i in (2, 5, 9, 7)]\n",
        "\n",
        "# unpack the weights for nicer assignment\n",
        "w1, w2, w3, w4 = weights\n",
        "\n",
        "b = w1 * a #8\n",
        "print(w1)\n",
        "c = w2 * a #20\n",
        "d = w3 * b + w4 * c # 72 + 140\n",
        "L = (10 - d) # 202\n",
        "\n",
        "L.backward()\n",
        "\n",
        "for index, weight in enumerate(weights, start=1):\n",
        "    gradient, *_ = weight.grad.data\n",
        "    print(f\"Gradient of w{index} w.r.t to L: {gradient}\")\n",
        "\n",
        "print(weights[0].grad)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2.], requires_grad=True)\n",
            "Gradient of w1 w.r.t to L: -36.0\n",
            "Gradient of w2 w.r.t to L: -28.0\n",
            "Gradient of w3 w.r.t to L: -8.0\n",
            "Gradient of w4 w.r.t to L: -20.0\n",
            "tensor([-36.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnqzJYtkCzDP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6358843d-f461-403f-b355-1a0d20c39be6"
      },
      "source": [
        "Variable(FloatTensor([4]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCcrZi5ZfaOe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "12c40a4d-3ac8-43c1-8bff-51f87ac45067"
      },
      "source": [
        "\n",
        "# ================================================================== #\n",
        "#                         Table of Contents                          #\n",
        "# ================================================================== #\n",
        "\n",
        "# 1. Basic autograd example 1               (Line 25 to 39)\n",
        "# 2. Basic autograd example 2               (Line 46 to 83)\n",
        "# 3. Loading data from numpy                (Line 90 to 97)\n",
        "# 4. Input pipline                          (Line 104 to 129)\n",
        "# 5. Input pipline for custom dataset       (Line 136 to 156)\n",
        "# 6. Pretrained model                       (Line 163 to 176)\n",
        "# 7. Save and load model                    (Line 183 to 189) \n",
        "\n",
        "\n",
        "# ================================================================== #\n",
        "#                     1. Basic autograd example 1                    #\n",
        "# ================================================================== #\n",
        "\n",
        "# Create tensors.\n",
        "x = torch.tensor(1., requires_grad=True)\n",
        "w = torch.tensor(2., requires_grad=True)\n",
        "b = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "# Build a computational graph.\n",
        "y = w * x + b    # y = 2 * x + 3\n",
        "\n",
        "# Compute gradients.\n",
        "y.backward()\n",
        "\n",
        "# Print out the gradients.\n",
        "print(x.grad)    # x.grad = 2 \n",
        "print(w.grad)    # w.grad = 1 \n",
        "print(b.grad)    # b.grad = 1 \n",
        "\n",
        "\n",
        "# ================================================================== #\n",
        "#                    2. Basic autograd example 2                     #\n",
        "# ================================================================== #\n",
        "\n",
        "# Create tensors of shape (10, 3) and (10, 2).\n",
        "x = torch.randn(10, 3)\n",
        "y = torch.randn(10, 2)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(3, 2)\n",
        "print ('w: ', linear.weight)\n",
        "print ('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "# Forward pass.\n",
        "pred = linear(x)\n",
        "\n",
        "# Compute loss.\n",
        "loss = criterion(pred, y)\n",
        "print('loss: ', loss.item())\n",
        "\n",
        "# Backward pass.\n",
        "loss.backward()\n",
        "\n",
        "# Print out the gradients.\n",
        "print ('dL/dw: ', linear.weight.grad) \n",
        "print ('dL/db: ', linear.bias.grad)\n",
        "\n",
        "# 1-step gradient descent.\n",
        "optimizer.step()\n",
        "\n",
        "# You can also perform gradient descent at the low level.\n",
        "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
        "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
        "\n",
        "# Print out the loss after 1-step gradient descent.\n",
        "pred = linear(x)\n",
        "loss = criterion(pred, y)\n",
        "print('loss after 1 step optimization: ', loss.item())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.)\n",
            "tensor(1.)\n",
            "tensor(1.)\n",
            "w:  Parameter containing:\n",
            "tensor([[ 0.4078, -0.2548, -0.5605],\n",
            "        [ 0.2656,  0.2738,  0.1337]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([0.1274, 0.1057], requires_grad=True)\n",
            "loss:  0.7412295341491699\n",
            "dL/dw:  tensor([[ 0.2082, -0.3198,  0.2160],\n",
            "        [ 0.0507, -0.1215,  0.1114]])\n",
            "dL/db:  tensor([0.0992, 0.5102])\n",
            "loss after 1 step optimization:  0.7363304495811462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpvroaJUfd24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5cc5204-3b2a-45fa-c621-595ad2975e83"
      },
      "source": [
        "# ================================================================== #\n",
        "#                         4. Input pipline                           #\n",
        "# ================================================================== #\n",
        "\n",
        "# Download and construct CIFAR-10 dataset.\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                             train=True, \n",
        "                                             transform=transforms.ToTensor(),\n",
        "                                             download=True)\n",
        "\n",
        "# Fetch one data pair (read data from disk).\n",
        "image, label = train_dataset[0]\n",
        "print (image.size())\n",
        "print (label)\n",
        "\n",
        "# Data loader (this provides queues and threads in a very simple way).\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=64, \n",
        "                                           shuffle=True)\n",
        "\n",
        "# When iteration starts, queue and thread start to load data from files.\n",
        "data_iter = iter(train_loader)\n",
        "\n",
        "# Mini-batch images and labels.\n",
        "images, labels = data_iter.next()\n",
        "\n",
        "# Actual usage of the data loader is as below.\n",
        "c = 0\n",
        "for images, labels in train_loader:\n",
        "    # Training code should be written here.\n",
        "    if c < 10:\n",
        "      print(labels)\n",
        "      c = c + 1\n",
        "    pass\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "  0%|          | 16384/170498071 [00:00<18:21, 154737.35it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Failed download. Trying https -> http instead. Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 221184/170498071 [00:00<13:19, 212910.87it/s]\u001b[A\n",
            "  0%|          | 425984/170498071 [00:00<09:44, 290741.91it/s]\u001b[A\n",
            "  0%|          | 548864/170498071 [00:00<08:06, 349358.37it/s]\u001b[A\n",
            "  0%|          | 638976/170498071 [00:00<07:13, 391744.79it/s]\u001b[A\n",
            "  1%|          | 999424/170498071 [00:00<05:16, 534724.76it/s]\u001b[A\n",
            "  1%|          | 1523712/170498071 [00:00<03:53, 724843.10it/s]\u001b[A\n",
            "  1%|          | 2048000/170498071 [00:01<02:52, 977558.06it/s]\u001b[A\n",
            "  2%|▏         | 2596864/170498071 [00:01<02:09, 1297445.79it/s]\u001b[A\n",
            "  2%|▏         | 3178496/170498071 [00:01<01:38, 1691418.62it/s]\u001b[A\n",
            "  2%|▏         | 3784704/170498071 [00:01<01:19, 2108036.17it/s]\u001b[A\n",
            "  3%|▎         | 4333568/170498071 [00:01<01:04, 2576355.05it/s]\u001b[A\n",
            "  3%|▎         | 4882432/170498071 [00:01<00:54, 3052705.26it/s]\u001b[A\n",
            "  3%|▎         | 5480448/170498071 [00:01<00:46, 3576385.73it/s]\u001b[A\n",
            "  4%|▎         | 6078464/170498071 [00:01<00:40, 4065556.36it/s]\u001b[A\n",
            "  4%|▍         | 6635520/170498071 [00:01<00:37, 4406051.16it/s]\u001b[A\n",
            "  4%|▍         | 7249920/170498071 [00:01<00:34, 4725459.81it/s]\u001b[A\n",
            "  5%|▍         | 7831552/170498071 [00:02<00:32, 4974939.21it/s]\u001b[A\n",
            "  5%|▍         | 8413184/170498071 [00:02<00:31, 5149134.85it/s]\u001b[A\n",
            "  5%|▌         | 9003008/170498071 [00:02<00:30, 5340982.97it/s]\u001b[A\n",
            "  6%|▌         | 9609216/170498071 [00:02<00:29, 5533148.68it/s]\u001b[A\n",
            "  6%|▌         | 10223616/170498071 [00:02<00:28, 5672006.42it/s]\u001b[A\n",
            "  6%|▋         | 10838016/170498071 [00:02<00:27, 5799810.42it/s]\u001b[A\n",
            "  7%|▋         | 11485184/170498071 [00:02<00:26, 5982972.72it/s]\u001b[A\n",
            "  7%|▋         | 12115968/170498071 [00:02<00:26, 6062599.60it/s]\u001b[A\n",
            "  8%|▊         | 12787712/170498071 [00:02<00:25, 6242935.73it/s]\u001b[A\n",
            "  8%|▊         | 13467648/170498071 [00:02<00:24, 6354720.05it/s]\u001b[A\n",
            "  8%|▊         | 14123008/170498071 [00:03<00:24, 6389155.20it/s]\u001b[A\n",
            "  9%|▊         | 14778368/170498071 [00:03<00:24, 6421015.44it/s]\u001b[A\n",
            "  9%|▉         | 15441920/170498071 [00:03<00:23, 6479052.90it/s]\u001b[A\n",
            "  9%|▉         | 16121856/170498071 [00:03<00:23, 6495610.11it/s]\u001b[A\n",
            " 10%|▉         | 16793600/170498071 [00:03<00:23, 6503248.67it/s]\u001b[A\n",
            " 10%|█         | 17506304/170498071 [00:03<00:22, 6677843.46it/s]\u001b[A\n",
            " 11%|█         | 18243584/170498071 [00:03<00:22, 6871712.91it/s]\u001b[A\n",
            " 11%|█         | 18980864/170498071 [00:03<00:21, 7000800.33it/s]\u001b[A\n",
            " 12%|█▏        | 19701760/170498071 [00:03<00:21, 7038944.54it/s]\u001b[A\n",
            " 12%|█▏        | 20439040/170498071 [00:03<00:21, 7042724.05it/s]\u001b[A\n",
            " 12%|█▏        | 21168128/170498071 [00:04<00:21, 7065821.86it/s]\u001b[A\n",
            " 13%|█▎        | 21880832/170498071 [00:04<00:21, 7049153.08it/s]\u001b[A\n",
            " 13%|█▎        | 22618112/170498071 [00:04<00:20, 7080907.45it/s]\u001b[A\n",
            " 14%|█▎        | 23363584/170498071 [00:04<00:20, 7183190.32it/s]\u001b[A\n",
            " 14%|█▍        | 24141824/170498071 [00:04<00:19, 7350454.43it/s]\u001b[A\n",
            " 15%|█▍        | 24879104/170498071 [00:04<00:20, 6988259.72it/s]\u001b[A\n",
            " 15%|█▌        | 25583616/170498071 [00:04<00:22, 6452516.40it/s]\u001b[A\n",
            " 15%|█▌        | 26247168/170498071 [00:04<00:22, 6358145.50it/s]\u001b[A\n",
            " 16%|█▌        | 27017216/170498071 [00:04<00:21, 6692773.41it/s]\u001b[A\n",
            " 16%|█▋        | 27803648/170498071 [00:05<00:20, 6910291.76it/s]\u001b[A\n",
            " 17%|█▋        | 28639232/170498071 [00:05<00:19, 7200236.60it/s]\u001b[A\n",
            " 17%|█▋        | 29450240/170498071 [00:05<00:18, 7443125.38it/s]\u001b[A\n",
            " 18%|█▊        | 30253056/170498071 [00:05<00:18, 7591645.99it/s]\u001b[A\n",
            " 18%|█▊        | 31080448/170498071 [00:05<00:17, 7769952.27it/s]\u001b[A\n",
            " 19%|█▊        | 31907840/170498071 [00:05<00:17, 7908015.37it/s]\u001b[A\n",
            " 19%|█▉        | 32817152/170498071 [00:05<00:16, 8175152.07it/s]\u001b[A\n",
            " 20%|█▉        | 33644544/170498071 [00:05<00:16, 8179191.05it/s]\u001b[A\n",
            " 20%|██        | 34529280/170498071 [00:05<00:16, 8337539.56it/s]\u001b[A\n",
            " 21%|██        | 35381248/170498071 [00:05<00:16, 8352173.85it/s]\u001b[A\n",
            " 21%|██▏       | 36257792/170498071 [00:06<00:15, 8395872.40it/s]\u001b[A\n",
            " 22%|██▏       | 37109760/170498071 [00:06<00:15, 8384952.80it/s]\u001b[A\n",
            " 22%|██▏       | 37978112/170498071 [00:06<00:15, 8445030.86it/s]\u001b[A\n",
            " 23%|██▎       | 38895616/170498071 [00:06<00:15, 8650560.09it/s]\u001b[A\n",
            " 23%|██▎       | 39837696/170498071 [00:06<00:14, 8859296.97it/s]\u001b[A\n",
            " 24%|██▍       | 40771584/170498071 [00:06<00:14, 8975271.18it/s]\u001b[A\n",
            " 24%|██▍       | 41672704/170498071 [00:06<00:14, 8985426.24it/s]\u001b[A\n",
            " 25%|██▍       | 42573824/170498071 [00:06<00:14, 8899431.65it/s]\u001b[A\n",
            " 26%|██▌       | 43524096/170498071 [00:06<00:14, 8991409.25it/s]\u001b[A\n",
            " 26%|██▌       | 44433408/170498071 [00:06<00:13, 9006295.32it/s]\u001b[A\n",
            " 27%|██▋       | 45375488/170498071 [00:07<00:13, 9122122.15it/s]\u001b[A\n",
            " 27%|██▋       | 46301184/170498071 [00:07<00:13, 9072443.64it/s]\u001b[A\n",
            " 28%|██▊       | 47235072/170498071 [00:07<00:13, 9116864.15it/s]\u001b[A\n",
            " 28%|██▊       | 48209920/170498071 [00:07<00:13, 9292642.66it/s]\u001b[A\n",
            " 29%|██▉       | 49168384/170498071 [00:07<00:13, 9323482.63it/s]\u001b[A\n",
            " 29%|██▉       | 50192384/170498071 [00:07<00:12, 9576124.39it/s]\u001b[A\n",
            " 30%|███       | 51191808/170498071 [00:07<00:12, 9697814.80it/s]\u001b[A\n",
            " 31%|███       | 52174848/170498071 [00:07<00:12, 9736291.75it/s]\u001b[A\n",
            " 31%|███       | 53174272/170498071 [00:07<00:11, 9799188.61it/s]\u001b[A\n",
            " 32%|███▏      | 54165504/170498071 [00:07<00:11, 9774740.14it/s]\u001b[A\n",
            " 32%|███▏      | 55156736/170498071 [00:08<00:11, 9790670.75it/s]\u001b[A\n",
            " 33%|███▎      | 56139776/170498071 [00:08<00:11, 9790443.34it/s]\u001b[A\n",
            " 34%|███▎      | 57163776/170498071 [00:08<00:11, 9917209.57it/s]\u001b[A\n",
            " 34%|███▍      | 58195968/170498071 [00:08<00:11, 9987747.02it/s]\u001b[A\n",
            " 35%|███▍      | 59260928/170498071 [00:08<00:10, 10172024.65it/s]\u001b[A\n",
            " 35%|███▌      | 60366848/170498071 [00:08<00:10, 10404210.00it/s]\u001b[A\n",
            " 36%|███▌      | 61415424/170498071 [00:08<00:10, 10392814.30it/s]\u001b[A\n",
            " 37%|███▋      | 62496768/170498071 [00:08<00:10, 10416307.23it/s]\u001b[A\n",
            " 37%|███▋      | 63545344/170498071 [00:08<00:10, 10416070.66it/s]\u001b[A\n",
            " 38%|███▊      | 64593920/170498071 [00:08<00:10, 10411503.69it/s]\u001b[A\n",
            " 39%|███▊      | 65650688/170498071 [00:09<00:10, 10380976.91it/s]\u001b[A\n",
            " 39%|███▉      | 66764800/170498071 [00:09<00:09, 10588128.01it/s]\u001b[A\n",
            " 40%|███▉      | 67846144/170498071 [00:09<00:09, 10615485.38it/s]\u001b[A\n",
            " 40%|████      | 68919296/170498071 [00:09<00:09, 10637123.12it/s]\u001b[A\n",
            " 41%|████      | 70000640/170498071 [00:09<00:09, 10671464.28it/s]\u001b[A\n",
            " 42%|████▏     | 71122944/170498071 [00:09<00:09, 10812793.52it/s]\u001b[A\n",
            " 42%|████▏     | 72278016/170498071 [00:09<00:08, 10919041.69it/s]\u001b[A\n",
            " 43%|████▎     | 73465856/170498071 [00:09<00:08, 11186227.21it/s]\u001b[A\n",
            " 44%|████▍     | 74637312/170498071 [00:09<00:08, 11313401.42it/s]\u001b[A\n",
            " 44%|████▍     | 75808768/170498071 [00:09<00:08, 11398919.20it/s]\u001b[A\n",
            " 45%|████▌     | 76955648/170498071 [00:10<00:08, 11249914.75it/s]\u001b[A\n",
            " 46%|████▌     | 78110720/170498071 [00:10<00:08, 11260483.62it/s]\u001b[A\n",
            " 46%|████▋     | 79265792/170498071 [00:10<00:08, 11219156.03it/s]\u001b[A\n",
            " 47%|████▋     | 80412672/170498071 [00:10<00:08, 11171719.51it/s]\u001b[A\n",
            " 48%|████▊     | 81567744/170498071 [00:10<00:07, 11257286.81it/s]\u001b[A\n",
            " 49%|████▊     | 82747392/170498071 [00:10<00:07, 11397031.93it/s]\u001b[A\n",
            " 49%|████▉     | 83943424/170498071 [00:10<00:07, 11545699.77it/s]\u001b[A\n",
            " 50%|████▉     | 85139456/170498071 [00:10<00:07, 11662555.39it/s]\u001b[A\n",
            " 51%|█████     | 86327296/170498071 [00:10<00:07, 11544746.81it/s]\u001b[A\n",
            " 51%|█████▏    | 87523328/170498071 [00:11<00:07, 11549487.44it/s]\u001b[A\n",
            " 52%|█████▏    | 88719360/170498071 [00:11<00:07, 11595901.73it/s]\u001b[A\n",
            " 53%|█████▎    | 89939968/170498071 [00:11<00:06, 11771984.37it/s]\u001b[A\n",
            " 53%|█████▎    | 91185152/170498071 [00:11<00:06, 11954878.03it/s]\u001b[A\n",
            " 54%|█████▍    | 92463104/170498071 [00:11<00:06, 12187477.56it/s]\u001b[A\n",
            " 55%|█████▍    | 93716480/170498071 [00:11<00:06, 12285367.97it/s]\u001b[A\n",
            " 56%|█████▌    | 94961664/170498071 [00:11<00:06, 12319882.19it/s]\u001b[A\n",
            " 56%|█████▋    | 96215040/170498071 [00:11<00:06, 12348469.56it/s]\u001b[A\n",
            " 57%|█████▋    | 97542144/170498071 [00:11<00:05, 12605397.09it/s]\u001b[A\n",
            " 58%|█████▊    | 98828288/170498071 [00:11<00:05, 12672137.37it/s]\u001b[A\n",
            " 59%|█████▊    | 100122624/170498071 [00:12<00:05, 12627160.51it/s]\u001b[A\n",
            " 60%|█████▉    | 101457920/170498071 [00:12<00:05, 12834619.82it/s]\u001b[A\n",
            " 60%|██████    | 102875136/170498071 [00:12<00:05, 13204308.20it/s]\u001b[A\n",
            " 61%|██████    | 104226816/170498071 [00:12<00:04, 13293435.83it/s]\u001b[A\n",
            " 62%|██████▏   | 105660416/170498071 [00:12<00:04, 13584755.15it/s]\u001b[A\n",
            " 63%|██████▎   | 107126784/170498071 [00:12<00:04, 13748319.27it/s]\u001b[A\n",
            " 64%|██████▎   | 108576768/170498071 [00:12<00:04, 13924969.39it/s]\u001b[A\n",
            " 65%|██████▍   | 110141440/170498071 [00:12<00:04, 14398021.96it/s]\u001b[A\n",
            " 66%|██████▌   | 111738880/170498071 [00:12<00:03, 14796912.47it/s]\u001b[A\n",
            " 66%|██████▋   | 113278976/170498071 [00:12<00:03, 14912887.07it/s]\u001b[A\n",
            " 67%|██████▋   | 114868224/170498071 [00:13<00:03, 14993936.21it/s]\u001b[A\n",
            " 68%|██████▊   | 116580352/170498071 [00:13<00:03, 15574039.97it/s]\u001b[A\n",
            " 69%|██████▉   | 118243328/170498071 [00:13<00:03, 15843123.99it/s]\u001b[A\n",
            " 70%|███████   | 119939072/170498071 [00:13<00:03, 16006336.46it/s]\u001b[A\n",
            " 71%|███████▏  | 121683968/170498071 [00:13<00:02, 16411544.70it/s]\u001b[A\n",
            " 72%|███████▏  | 123502592/170498071 [00:13<00:02, 16886932.51it/s]\u001b[A\n",
            " 74%|███████▎  | 125353984/170498071 [00:13<00:02, 16956590.90it/s]\u001b[A\n",
            " 75%|███████▍  | 127229952/170498071 [00:13<00:02, 17458246.44it/s]\u001b[A\n",
            " 76%|███████▌  | 129155072/170498071 [00:13<00:02, 17861020.44it/s]\u001b[A\n",
            " 77%|███████▋  | 131153920/170498071 [00:13<00:02, 18428098.00it/s]\u001b[A\n",
            " 78%|███████▊  | 133218304/170498071 [00:14<00:01, 19015654.78it/s]\u001b[A\n",
            " 79%|███████▉  | 135274496/170498071 [00:14<00:01, 19175338.99it/s]\u001b[A\n",
            " 81%|████████  | 137437184/170498071 [00:14<00:01, 19839947.57it/s]\u001b[A\n",
            " 82%|████████▏ | 139608064/170498071 [00:14<00:01, 20307102.63it/s]\u001b[A\n",
            " 83%|████████▎ | 141819904/170498071 [00:14<00:01, 20665264.07it/s]\u001b[A\n",
            " 84%|████████▍ | 144056320/170498071 [00:14<00:01, 20852446.97it/s]\u001b[A\n",
            " 86%|████████▌ | 146350080/170498071 [00:14<00:01, 21413787.24it/s]\u001b[A\n",
            " 87%|████████▋ | 148742144/170498071 [00:14<00:00, 21903589.46it/s]\u001b[A\n",
            " 89%|████████▊ | 151142400/170498071 [00:14<00:00, 22475789.91it/s]\u001b[A\n",
            " 90%|█████████ | 153640960/170498071 [00:14<00:00, 23085492.70it/s]\u001b[A\n",
            " 92%|█████████▏| 156164096/170498071 [00:15<00:00, 23569366.98it/s]\u001b[A\n",
            " 93%|█████████▎| 158769152/170498071 [00:15<00:00, 24262078.30it/s]\u001b[A\n",
            " 95%|█████████▍| 161406976/170498071 [00:15<00:00, 24754042.51it/s]\u001b[A\n",
            " 96%|█████████▋| 164143104/170498071 [00:15<00:00, 25477403.97it/s]\u001b[A\n",
            " 98%|█████████▊| 166936576/170498071 [00:15<00:00, 26155398.85it/s]\u001b[A\n",
            "100%|█████████▉| 169738240/170498071 [00:15<00:00, 26680067.72it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../../data/cifar-10-python.tar.gz to ../../data/\n",
            "torch.Size([3, 32, 32])\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPlXpMNPq6yA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "b5ce7958-015b-4781-ab55-fc9f32790529"
      },
      "source": [
        "c = 0\n",
        "for images, labels in train_loader:\n",
        "    # Training code should be written here.\n",
        "    if c < 10:\n",
        "      print(images[0].size())\n",
        "      c = c + 1\n",
        "    pass"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n",
            "torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VQLL1pGf0Ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# ================================================================== #\n",
        "#                5. Input pipline for custom dataset                 #\n",
        "# ================================================================== #\n",
        "\n",
        "# You should build your custom dataset as below.\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "        # TODO\n",
        "        # 1. Initialize file paths or a list of file names. \n",
        "        pass\n",
        "    def __getitem__(self, index):\n",
        "        # TODO\n",
        "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
        "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
        "        # 3. Return a data pair (e.g. image and label).\n",
        "        pass\n",
        "    def __len__(self):\n",
        "        # You should change 0 to the total size of your dataset.\n",
        "        return 0 \n",
        "\n",
        "# You can then use the prebuilt data loader. \n",
        "custom_dataset = CustomDataset()\n",
        "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
        "                                           batch_size=64, \n",
        "                                           shuffle=True)\n",
        "\n",
        "\n",
        "# ================================================================== #\n",
        "#                        6. Pretrained model                         #\n",
        "# ================================================================== #\n",
        "\n",
        "# Download and load the pretrained ResNet-18.\n",
        "resnet = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# If you want to finetune only the top layer of the model, set as below.\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the top layer for finetuning.\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n",
        "\n",
        "# Forward pass.\n",
        "images = torch.randn(64, 3, 224, 224)\n",
        "outputs = resnet(images)\n",
        "print (outputs.size())     # (64, 100)\n",
        "\n",
        "\n",
        "# ================================================================== #\n",
        "#                      7. Save and load the model                    #\n",
        "# ================================================================== #\n",
        "\n",
        "# Save and load the entire model.\n",
        "torch.save(resnet, 'model.ckpt')\n",
        "model = torch.load('model.ckpt')\n",
        "\n",
        "# Save and load only the model parameters (recommended).\n",
        "torch.save(resnet.state_dict(), 'params.ckpt')\n",
        "resnet.load_state_dict(torch.load('params.ckpt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKbr5nA6ZJ9T",
        "colab_type": "text"
      },
      "source": [
        "Simple Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iRJKD_pZMnT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "e048c44b-0b92-412f-90d1-bd77b5c8d09a"
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    #Conv layers\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "    # FC Layers\n",
        "    self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "    self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(self.conv1(x), (2,2))\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "    x = x.view(-1, self.num_flat_features(x))\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "  def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "net = NeuralNet()\n",
        "print(net)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNet(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA7IoUYGtOWt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ab146ab-6455-4311-c06f-c6e40ea287d1"
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(params[0].size())"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6, 1, 5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpEYYwn5uJvJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9058b6f2-414e-40d5-87c0-7e91113778f9"
      },
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0535,  0.1233, -0.0203,  0.0653, -0.0725, -0.0901,  0.0750,  0.0137,\n",
            "          0.0887, -0.1660]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGK0OdAyuXFq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "338a28dc-324b-46ed-ceda-71f465c76ede"
      },
      "source": [
        "#Loss computation\n",
        "\n",
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.0692, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r51dbNAK3RDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "89d3327e-ec96-4b24-d674-72d887e8dc2e"
      },
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7f0360654860>\n",
            "<AddmmBackward object at 0x7f0360654550>\n",
            "<AccumulateGrad object at 0x7f0360654860>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0xz3mDp3eb-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "outputId": "e3e5b75d-d7e5-4ba8-f6a8-968635618e82"
      },
      "source": [
        "net.zero_grad()\n",
        "\n",
        "print('conv1 grad before backprop')\n",
        "print(net.conv1.weight.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1 grad after backprop')\n",
        "print(net.conv1.weight.grad)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1 grad before backprop\n",
            "None\n",
            "conv1 grad after backprop\n",
            "tensor([[[[ 7.8256e-03, -1.2072e-03, -6.7794e-03, -1.0931e-02,  5.9057e-03],\n",
            "          [-3.6541e-03, -2.0363e-02,  1.1442e-02, -6.7667e-03,  1.1405e-02],\n",
            "          [ 1.2727e-02,  1.6590e-02,  2.0165e-03, -3.7445e-03,  2.3650e-02],\n",
            "          [ 1.5314e-03, -1.6798e-02,  1.2756e-02, -1.8046e-03, -1.3962e-02],\n",
            "          [ 7.4986e-03, -9.4742e-03,  2.2131e-03,  6.4103e-03,  2.6853e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.5541e-03, -8.4234e-03, -2.2605e-02, -6.0791e-03,  9.8344e-03],\n",
            "          [-1.1196e-02,  1.4949e-02,  7.3339e-03, -1.6946e-02, -2.5760e-03],\n",
            "          [-4.3093e-03,  3.5342e-04, -1.7460e-02,  3.1444e-02, -1.6968e-02],\n",
            "          [ 2.1975e-03,  1.8721e-02,  2.4375e-02, -6.5204e-03, -1.5079e-02],\n",
            "          [-4.6113e-03,  4.8170e-05, -1.2978e-03, -6.7232e-03,  1.4883e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.5062e-02,  1.4291e-02,  3.0478e-03, -4.0698e-03, -9.4697e-03],\n",
            "          [-2.8825e-03,  2.2096e-02,  3.7767e-03,  3.2314e-03,  2.6987e-03],\n",
            "          [ 7.1559e-03, -6.9298e-03, -5.2414e-03,  1.2822e-02,  7.1198e-03],\n",
            "          [-1.4206e-02,  1.0393e-02, -2.3536e-02,  1.6340e-02, -1.1956e-02],\n",
            "          [-2.1717e-02,  7.5455e-03,  3.1553e-03,  2.9092e-03,  7.3470e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 9.7203e-04,  2.7255e-02, -8.5877e-03,  1.1534e-04, -1.4368e-02],\n",
            "          [ 9.3772e-03,  2.2861e-02,  7.6129e-03, -3.3252e-03, -1.2522e-02],\n",
            "          [-2.9854e-03,  9.0714e-03,  3.7104e-05,  2.3311e-03,  1.1348e-02],\n",
            "          [ 6.8720e-03,  6.4853e-03, -4.7159e-03,  1.1249e-02, -1.0741e-02],\n",
            "          [ 5.9494e-03,  2.3002e-03,  1.0273e-02, -6.9084e-03, -8.1824e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.7835e-03,  3.8496e-02, -3.0142e-03,  6.8871e-03,  2.0206e-03],\n",
            "          [ 2.2376e-02,  2.0113e-02, -2.5695e-02,  1.1566e-02, -8.2855e-03],\n",
            "          [-1.6297e-02,  1.4460e-02, -8.0669e-03,  2.1468e-02, -4.8534e-03],\n",
            "          [ 5.9170e-04, -1.2373e-03,  4.3457e-03, -3.7838e-03, -1.1197e-02],\n",
            "          [ 1.8190e-02, -2.1391e-02, -6.0748e-03,  5.2840e-03, -1.7862e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.3171e-03,  1.0867e-02,  8.2739e-03,  2.6557e-02,  4.4577e-04],\n",
            "          [ 6.8404e-03,  2.2165e-02,  1.0257e-03,  2.8636e-02, -6.4645e-03],\n",
            "          [ 9.7731e-03,  8.0519e-03,  1.0066e-03,  9.6024e-03, -1.4791e-02],\n",
            "          [ 3.3312e-04,  2.0501e-02,  8.9636e-04, -1.1446e-02, -4.7314e-03],\n",
            "          [-1.1726e-02, -9.8922e-03, -2.0733e-03,  2.0289e-03, -7.5921e-03]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmkGHl1e4Oyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.003\n",
        "for param in net.parameters():\n",
        "  param.data.sub_(param.grad.data * lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvIDfpx93ylt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.003)\n",
        "\n",
        "#Training loop:\n",
        "\n",
        "optimizer.zero_grad()\n",
        "output = net(input)\n",
        "loss = criterion(output, target) \n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HhspfDZ5gvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "da7581a3-5cc1-4530-ee93-bdcf14caac58"
      },
      "source": [
        "print('conv1 grad after SGD backprop')\n",
        "print(net.conv1.weight.grad)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1 grad after SGD backprop\n",
            "tensor([[[[ 7.7669e-03, -1.2543e-03, -6.7836e-03, -1.0715e-02,  5.9997e-03],\n",
            "          [-3.7381e-03, -2.0054e-02,  1.1414e-02, -6.7713e-03,  1.1561e-02],\n",
            "          [ 1.2980e-02,  1.6440e-02,  2.1325e-03, -3.5988e-03,  2.3671e-02],\n",
            "          [ 1.5571e-03, -1.6780e-02,  1.2497e-02, -1.6615e-03, -1.4283e-02],\n",
            "          [ 7.3560e-03, -9.5862e-03,  2.0173e-03,  6.2795e-03,  2.7221e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.4927e-03, -8.4265e-03, -2.2298e-02, -6.0706e-03,  9.7308e-03],\n",
            "          [-1.1127e-02,  1.5295e-02,  7.3066e-03, -1.6987e-02, -2.5105e-03],\n",
            "          [-3.9256e-03,  4.3962e-04, -1.7427e-02,  3.1461e-02, -1.7096e-02],\n",
            "          [ 2.0151e-03,  1.8796e-02,  2.4462e-02, -6.4947e-03, -1.5197e-02],\n",
            "          [-4.5610e-03, -3.9045e-04, -1.4335e-03, -7.0181e-03,  1.4520e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.5372e-02,  1.4035e-02,  2.8614e-03, -3.9151e-03, -9.1778e-03],\n",
            "          [-3.2690e-03,  2.2555e-02,  3.3456e-03,  3.8937e-03,  2.8667e-03],\n",
            "          [ 6.9712e-03, -7.2250e-03, -5.0787e-03,  1.2918e-02,  6.7779e-03],\n",
            "          [-1.4204e-02,  1.0755e-02, -2.3960e-02,  1.5973e-02, -1.2312e-02],\n",
            "          [-2.2021e-02,  7.5826e-03,  3.4760e-03,  2.5734e-03,  7.7362e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2888e-03,  2.7600e-02, -8.4087e-03, -1.1366e-05, -1.4396e-02],\n",
            "          [ 9.1532e-03,  2.3190e-02,  7.8086e-03, -3.5082e-03, -1.2197e-02],\n",
            "          [-2.7970e-03,  8.8268e-03, -2.8333e-04,  2.6514e-03,  1.1439e-02],\n",
            "          [ 6.7424e-03,  6.6568e-03, -4.3252e-03,  1.1046e-02, -1.0502e-02],\n",
            "          [ 5.9403e-03,  2.8539e-03,  9.8684e-03, -7.1327e-03, -8.3402e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3624e-03,  3.8866e-02, -2.5177e-03,  6.7911e-03,  2.3441e-03],\n",
            "          [ 2.3101e-02,  2.0394e-02, -2.6828e-02,  1.2621e-02, -9.0883e-03],\n",
            "          [-1.7084e-02,  1.4787e-02, -7.7393e-03,  2.1453e-02, -5.5411e-03],\n",
            "          [ 6.8746e-04, -1.3371e-03,  4.8572e-03, -4.6922e-03, -1.0842e-02],\n",
            "          [ 1.8531e-02, -2.1794e-02, -6.5517e-03,  5.9086e-03, -1.7640e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.4210e-03,  1.1012e-02,  8.5124e-03,  2.6266e-02,  9.3592e-04],\n",
            "          [ 7.2148e-03,  2.2494e-02,  8.6021e-04,  2.8728e-02, -6.7305e-03],\n",
            "          [ 9.9424e-03,  8.0115e-03,  8.2574e-04,  9.7786e-03, -1.4960e-02],\n",
            "          [ 7.4503e-05,  2.0285e-02,  8.2285e-04, -1.1279e-02, -4.6567e-03],\n",
            "          [-1.2116e-02, -9.7876e-03, -2.0020e-03,  2.1825e-03, -7.8143e-03]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}